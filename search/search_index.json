{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"\ud83e\udd8a FE-Neck","text":"<p>Feature extraction neck modules for computer vision models. This library provides a comprehensive collection of feature pyramid network architectures for multi-scale feature fusion in object detection, segmentation, and other computer vision tasks.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code># Clone the repository\ngit clone https://github.com/vncntcmnn/feneck.git\ncd feneck\n\n# Install dependencies (CPU version)\nuv sync --extra cpu\n\n# Or with CUDA support\nuv sync --extra cu118  # CUDA 11.8\nuv sync --extra cu124  # CUDA 12.4\n</code></pre>"},{"location":"#quick-start","title":"Quick Start","text":"<p>All necks can be imported directly from the <code>feneck</code> package:</p> <pre><code>import torch\nfrom feneck import FPN, PAFPN, BiFPN\n\n# Create a Feature Pyramid Network\nfpn = FPN(\n    in_channels=[256, 512, 1024, 2048],\n    in_strides=[4, 8, 16, 32],\n    out_channels=256\n)\n\n# Forward pass with backbone features\nbackbone_features = [\n    torch.randn(1, 256, 64, 64),   # stride 4\n    torch.randn(1, 512, 32, 32),   # stride 8\n    torch.randn(1, 1024, 16, 16),  # stride 16\n    torch.randn(1, 2048, 8, 8),    # stride 32\n]\n\npyramid_features = fpn(backbone_features)\n# Output: 5 levels with 256 channels each\n</code></pre>"},{"location":"#available-modules","title":"Available Modules","text":""},{"location":"#standard-fpn-variants","title":"Standard FPN Variants","text":""},{"location":"#fpn-feature-pyramid-network","title":"FPN - Feature Pyramid Network","text":"<p>Classic top-down architecture with lateral connections for multi-scale feature fusion.</p> <pre><code>from feneck import FPN\n\nfpn = FPN(\n    in_channels=[256, 512, 1024, 2048],\n    in_strides=[4, 8, 16, 32],\n    out_channels=256\n)\n</code></pre> <p>Use Case: Standard multi-scale object detection</p>"},{"location":"#pafpn-path-aggregation-fpn","title":"PAFPN - Path Aggregation FPN","text":"<p>Enhances FPN with an additional bottom-up pathway for better feature propagation.</p> <pre><code>from feneck import PAFPN\n\npafpn = PAFPN(\n    in_channels=[256, 512, 1024],\n    in_strides=[8, 16, 32],\n    out_channels=256\n)\n</code></pre> <p>Use Case: Enhanced feature fusion for detection tasks</p>"},{"location":"#bifpn-bidirectional-feature-pyramid-network","title":"BiFPN - Bidirectional Feature Pyramid Network","text":"<p>Efficient bidirectional cross-scale connections with learnable weights.</p> <pre><code>from feneck import BiFPN\n\nbifpn = BiFPN(\n    in_channels=[256, 512, 1024],\n    in_strides=[8, 16, 32],\n    out_channels=256\n)\n</code></pre> <p>Use Case: Efficient multi-scale fusion with weighted connections</p>"},{"location":"#nasfpn-neural-architecture-search-fpn","title":"NASFPN - Neural Architecture Search FPN","text":"<p>Feature pyramid architecture discovered through neural architecture search.</p> <pre><code>from feneck import NASFPN\n\nnasfpn = NASFPN(\n    in_channels=[256, 512, 1024],\n    in_strides=[8, 16, 32],\n    out_channels=256\n)\n</code></pre> <p>Use Case: Learned fusion patterns for optimal performance</p>"},{"location":"#specialized-architectures","title":"Specialized Architectures","text":""},{"location":"#simplefpn","title":"SimpleFPN","text":"<p>Designed for transformer backbones that output single-scale features.</p> <pre><code>from feneck import SimpleFPN\n\nsimple_fpn = SimpleFPN(\n    in_channels=768,        # ViT output channels\n    in_strides=16,          # ViT patch size\n    out_channels=256,\n    start_level=2\n)\n</code></pre> <p>Use Case: Converting single-scale Vision Transformer outputs to multi-scale features</p>"},{"location":"#customcsppan","title":"CustomCSPPAN","text":"<p>CSP-PAN architecture with optional transformer enhancement.</p> <pre><code>from feneck import CustomCSPPAN\n\ncsp_pan = CustomCSPPAN(\n    in_channels=[256, 512, 1024],\n    in_strides=[8, 16, 32],\n    out_channels=256,\n    use_transformer=True\n)\n</code></pre> <p>Use Case: Advanced feature aggregation with attention mechanisms</p>"},{"location":"#hrfpn-high-resolution-fpn","title":"HRFPN - High-Resolution FPN","text":"<p>Maintains high-resolution representations throughout the network.</p> <pre><code>from feneck import HRFPN\n\nhrfpn = HRFPN(\n    in_channels=[256, 512, 1024],\n    in_strides=[8, 16, 32],\n    out_channels=256\n)\n</code></pre> <p>Use Case: Tasks requiring fine-grained spatial details</p>"},{"location":"#lrfpn-location-refined-fpn","title":"LRFPN - Location-Refined FPN","text":"<p>Specialized for remote sensing and aerial image object detection.</p> <pre><code>from feneck import LRFPN\n\nlrfpn = LRFPN(\n    in_channels=[256, 512, 1024],  # shallow, F2, F3\n    in_strides=[4, 8, 16],\n    out_channels=256\n)\n</code></pre> <p>Use Case: Remote sensing object detection with location refinement</p>"},{"location":"#feature-enhancement-modules","title":"Feature Enhancement Modules","text":""},{"location":"#carafe-content-aware-reassembly-of-features","title":"CARAFE - Content-Aware ReAssembly of FEatures","text":"<p>Content-aware upsampling for better feature reconstruction.</p> <pre><code>from feneck import CARAFE\n\ncarafe = CARAFE(\n    in_channels=[256, 512, 1024],\n    in_strides=[8, 16, 32],\n    out_channels=256\n)\n</code></pre> <p>Use Case: High-quality feature upsampling with content awareness</p>"},{"location":"#dyhead-dynamic-head","title":"DyHead - Dynamic Head","text":"<p>Post-FPN refinement with scale-aware, spatial-aware, and task-aware attention.</p> <pre><code>from feneck import DyHead\n\n# Requires uniform input channels\ndyhead = DyHead(\n    in_channels=[256, 256, 256],\n    in_strides=[8, 16, 32],\n    out_channels=256\n)\n</code></pre> <p>Use Case: Post-processing FPN features with dynamic attention</p>"},{"location":"#utility-modules","title":"Utility Modules","text":""},{"location":"#featurepyramidextender","title":"FeaturePyramidExtender","text":"<p>Preprocesses backbone features by extending pyramid levels and unifying channels.</p> <pre><code>from feneck import FeaturePyramidExtender\n\nextender = FeaturePyramidExtender(\n    in_channels=[256, 512, 1024],\n    in_strides=[8, 16, 32],\n    out_channels=256,\n    num_levels=5  # Extend to 5 pyramid levels\n)\n</code></pre> <p>Use Case: Adapting backbone outputs for specific neck requirements</p>"},{"location":"#architecture-compatibility","title":"Architecture Compatibility","text":"Backbone Type Recommended Necks ResNet, RegNet, EfficientNet FPN, PAFPN, BiFPN, NASFPN, CustomCSPPAN, HRFPN, LRFPN, CARAFE Vision Transformer (ViT, Swin) SimpleFPN Any backbone FeaturePyramidExtender (preprocessing) Post-FPN processing DyHead"},{"location":"#common-patterns","title":"Common Patterns","text":""},{"location":"#hierarchical-backbones-resnet-etc","title":"Hierarchical Backbones (ResNet, etc.)","text":"<pre><code>from feneck import FPN\n\n# Standard FPN usage\nfpn = FPN(\n    in_channels=[256, 512, 1024, 2048],  # C2, C3, C4, C5\n    in_strides=[4, 8, 16, 32],\n    out_channels=256\n)\n</code></pre>"},{"location":"#transformer-backbones","title":"Transformer Backbones","text":"<pre><code>from feneck import SimpleFPN\n\n# Convert single-scale to multi-scale\nsimple_fpn = SimpleFPN(\n    in_channels=768,\n    in_strides=16,\n    out_channels=256,\n    start_level=2\n)\n</code></pre>"},{"location":"#extending-pyramid-levels","title":"Extending Pyramid Levels","text":"<pre><code>from feneck import FeaturePyramidExtender, PAFPN\n\n# Preprocess then apply neck\nextender = FeaturePyramidExtender(\n    in_channels=[256, 512, 1024],\n    in_strides=[8, 16, 32],\n    out_channels=256,\n    num_levels=5\n)\n\npafpn = PAFPN(\n    in_channels=[256, 256, 256, 256, 256],\n    in_strides=[4, 8, 16, 32, 64],\n    out_channels=256\n)\n</code></pre>"},{"location":"#requirements","title":"Requirements","text":"<ul> <li>Python \u2265 3.10</li> <li>PyTorch \u2265 2.0.0</li> <li>torchvision \u2265 0.15.0</li> </ul>"},{"location":"#license","title":"License","text":"<p>Apache License 2.0</p> <p>Some implementations adapted from: - PaddleDetection (Apache 2.0) - Microsoft Research - Google Research</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this library in your research, please cite:</p> <pre><code>@software{feneck2025,\n  author = {Camenen, Vincent},\n  title = {FE-Neck: Feature Extraction Neck Modules},\n  year = {2025},\n  url = {https://github.com/vncntcmnn/feneck}\n}\n</code></pre>"},{"location":"modules/","title":"API Reference","text":""},{"location":"modules/#feneck","title":"<code>feneck</code>","text":""},{"location":"modules/#feneck.BiFPN","title":"<code>BiFPN</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Bidirectional Feature Pyramid Network for efficient multi-scale feature fusion.</p> <p>BiFPN introduces weighted bidirectional cross-scale connections for improved feature fusion compared to traditional FPN and PANet. Key innovations include fast normalized fusion, same-level skip connections, and depthwise separable convolutions.</p> <p>Reference</p> <p>EfficientDet: Scalable and Efficient Object Detection Tan et al., CVPR 2020  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels)</p> <code>256</code> <code>num_levels</code> <code>int</code> <p>Total number of pyramid levels</p> <code>5</code> <code>num_layers</code> <code>int</code> <p>Number of BiFPN blocks to stack</p> <code>3</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to all convolution layers</p> <code>None</code> <code>use_fast_attention</code> <code>bool</code> <p>Whether to use fast attention for feature fusion</p> <code>True</code> <code>epsilon</code> <code>float</code> <p>Small value for numerical stability in attention normalization</p> <code>0.0001</code>"},{"location":"modules/#feneck.CARAFE","title":"<code>CARAFE</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>CARAFE Feature Pyramid Network neck module.</p> <p>Replaces standard FPN upsampling with Content-Aware ReAssembly of FEatures (CARAFE) for improved feature upsampling and multi-scale feature fusion.</p> <p>This implementation uses standard PyTorch operations without CUDA dependencies.</p> <p>Reference</p> <p>CARAFE: Content-Aware ReAssembly of FEatures Wang et al., ICCV 2019  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level (must be increasing)</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels)</p> <code>256</code> <code>num_levels</code> <code>int | None</code> <p>Total number of output pyramid levels. If None, uses len(in_channels). Must be &gt;= len(in_channels)</p> <code>None</code> <code>has_extra_convs</code> <code>bool</code> <p>Whether to add extra conv layers for additional pyramid levels. False uses max pooling for one extra level (Faster R-CNN style). True uses strided convolutions for extra levels (RetinaNet/FCOS style)</p> <code>False</code> <code>use_c5</code> <code>bool</code> <p>Whether to use backbone's highest feature (c5) as input for first extra level. False uses FPN's highest feature (p5) instead. Only affects extra convolutions</p> <code>True</code> <code>carafe_scale_factor</code> <code>int</code> <p>Scale factor for CARAFE upsampling</p> <code>2</code> <code>carafe_up_kernel</code> <code>int</code> <p>Kernel size for CARAFE reassembly operation</p> <code>5</code> <code>carafe_encoder_kernel</code> <code>int</code> <p>Kernel size for CARAFE content encoder</p> <code>3</code> <code>carafe_encoder_dilation</code> <code>int</code> <p>Dilation for CARAFE content encoder</p> <code>1</code> <code>carafe_compressed_channels</code> <code>int</code> <p>Intermediate channels in CARAFE</p> <code>64</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to all convolution layers</p> <code>None</code> <code>relu_before_extra_convs</code> <code>bool</code> <p>Whether to apply ReLU activation before extra convolutions</p> <code>True</code>"},{"location":"modules/#feneck.CustomCSPPAN","title":"<code>CustomCSPPAN</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Custom CSP-PAN neck with transformer enhancement.</p> <p>Adapted from PaddleDetection CustomCSPPAN implementation.</p> <p>Reference</p> <p>PP-YOLOv2: A Practical Object Detector Huang et al., 2021  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels)</p> <code>256</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to all convolution layers</p> <code>None</code> <code>activation</code> <code>str</code> <p>Activation function type</p> <code>'relu'</code> <code>stage_num</code> <code>int</code> <p>Number of stages in each CSP block</p> <code>1</code> <code>block_num</code> <code>int</code> <p>Number of basic blocks in each CSP stage</p> <code>3</code> <code>spp</code> <code>bool</code> <p>Whether to use SPP (Spatial Pyramid Pooling) in the deepest level</p> <code>False</code> <code>use_alpha</code> <code>bool</code> <p>Whether to use learnable alpha parameter in residual connections</p> <code>False</code> <code>width_mult</code> <code>float</code> <p>Width multiplier for channel scaling</p> <code>1.0</code> <code>depth_mult</code> <code>float</code> <p>Depth multiplier for block number scaling</p> <code>1.0</code> <code>use_transformer</code> <code>bool</code> <p>Whether to use transformer enhancement on deepest feature</p> <code>False</code> <code>transformer_num_heads</code> <code>int</code> <p>Number of attention heads in transformer</p> <code>4</code> <code>transformer_num_layers</code> <code>int</code> <p>Number of transformer encoder layers</p> <code>4</code> <code>transformer_dim_feedforward</code> <code>int</code> <p>Feedforward dimension in transformer</p> <code>2048</code> <code>transformer_dropout</code> <code>float</code> <p>Dropout rate in transformer</p> <code>0.1</code>"},{"location":"modules/#feneck.DyHead","title":"<code>DyHead</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Dynamic Head neck with attention mechanisms for object detection.</p> <p>Adapted from Microsoft's DynamicHead implementation with three types of attention: - Scale-aware attention: Combines features from different pyramid levels - Spatial-aware attention: Uses deformable convolutions for spatial modeling - Task-aware attention: Uses dynamic ReLU for channel-wise adaptive activation</p> <p>DyHead is typically used after an FPN neck, which is why all input levels are required to have the same number of channels (standard FPN output).</p> <p>Reference</p> <p>Dynamic Head: Unifying Object Detection Heads with Attentions Dai et al., CVPR 2021  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level (must be same for all levels)</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels)</p> <code>256</code> <code>num_blocks</code> <code>int</code> <p>Number of DyHead blocks to stack</p> <code>6</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to convolutions</p> <code>'group'</code>"},{"location":"modules/#feneck.FPN","title":"<code>FPN</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Feature Pyramid Network neck module.</p> <p>Reference</p> <p>Feature Pyramid Networks for Object Detection Lin et al., CVPR 2017  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level (must be increasing)</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels)</p> <code>256</code> <code>num_levels</code> <code>int | None</code> <p>Total number of output pyramid levels. If None, uses len(in_channels). Must be &gt;= len(in_channels)</p> <code>None</code> <code>has_extra_convs</code> <code>bool</code> <p>Whether to add extra conv layers for additional pyramid levels. False uses max pooling for one extra level (Faster R-CNN style). True uses strided convolutions for extra levels (RetinaNet/FCOS style)</p> <code>False</code> <code>use_c5</code> <code>bool</code> <p>Whether to use backbone's highest feature (c5) as input for first extra level. False uses FPN's highest feature (p5) instead. Only affects extra convolutions</p> <code>True</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to all convolution layers</p> <code>None</code> <code>relu_before_extra_convs</code> <code>bool</code> <p>Whether to apply ReLU activation before extra convolutions</p> <code>True</code>"},{"location":"modules/#feneck.FeaturePyramidExtender","title":"<code>FeaturePyramidExtender</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Extends and transforms feature pyramids by adding levels and optionally projecting channels.</p> <p>This neck can:</p> <ol> <li>Add extra feature levels at higher or lower resolutions</li> <li>Optionally project all features to uniform channel count</li> <li>Or do both simultaneously</li> </ol> <p>Useful for preprocessing before other necks that need specific level counts or channel uniformity, or for extending necks like CustomCSPPAN that can't create additional levels themselves.</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level (must be increasing)</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels). If None, keeps original channels</p> <code>256</code> <code>num_levels</code> <code>int | None</code> <p>Total number of output levels. If None, uses len(in_channels)</p> <code>None</code> <code>add_higher_res</code> <code>bool</code> <p>Whether to add higher resolution levels (lower strides) at the beginning. If False, only adds lower resolution levels (higher strides) at the end</p> <code>False</code> <code>project_channels</code> <code>bool</code> <p>Whether to project input features to out_channels. If False, only extra levels use out_channels while original features keep their channels</p> <code>True</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to convolution layers</p> <code>None</code> <code>relu_before_downsample</code> <code>bool</code> <p>Whether to apply ReLU before downsampling convolutions</p> <code>True</code> Usage Examples <pre><code>&gt;&gt;&gt; # Just add levels, keep original channels\n&gt;&gt;&gt; extender = FeaturePyramidExtender(\n...     in_channels=[256, 512, 1024],\n...     in_strides=[8, 16, 32],\n...     out_channels=256,  # Only for new levels\n...     num_levels=5,\n...     project_channels=False\n... )\n&gt;&gt;&gt; # Output channels: [256, 512, 1024, 256, 256]\n\n&gt;&gt;&gt; # Add levels + unify channels\n&gt;&gt;&gt; extender = FeaturePyramidExtender(\n...     in_channels=[256, 512, 1024],\n...     in_strides=[8, 16, 32],\n...     out_channels=256,\n...     num_levels=5,\n...     project_channels=True  # Project all to 256 channels\n... )\n&gt;&gt;&gt; # Output channels: [256, 256, 256, 256, 256]\n\n&gt;&gt;&gt; # Just unify channels without adding levels\n&gt;&gt;&gt; extender = FeaturePyramidExtender(\n...     in_channels=[256, 512, 1024],\n...     in_strides=[8, 16, 32],\n...     out_channels=256,\n...     project_channels=True\n... )\n&gt;&gt;&gt; # Output channels: [256, 256, 256]\n\n&gt;&gt;&gt; # Add higher resolution levels\n&gt;&gt;&gt; extender = FeaturePyramidExtender(\n...     in_channels=[512, 1024],\n...     in_strides=[16, 32],\n...     out_channels=256,\n...     num_levels=4,  # Add P2 (stride 4) and P3 (stride 8)\n...     add_higher_res=True\n... )\n&gt;&gt;&gt; # Output strides: [4, 8, 16, 32]\n</code></pre>"},{"location":"modules/#feneck.FeaturePyramidExtender.out_channels","title":"<code>out_channels</code>  <code>property</code>","text":"<p>Output channels for each feature level.</p>"},{"location":"modules/#feneck.FeaturePyramidExtender.out_strides","title":"<code>out_strides</code>  <code>property</code>","text":"<p>Output strides for each feature level.</p>"},{"location":"modules/#feneck.HRFPN","title":"<code>HRFPN</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>High-Resolution Feature Pyramid Network neck module.</p> <p>HRFPN aggregates multi-resolution feature representations by upsampling all feature levels to the highest resolution, concatenating them, then generating pyramid levels through pooling. Originally designed for HRNet but works with any multi-scale backbone.</p> <p>Reference</p> <p>Deep High-Resolution Representation Learning for Visual Recognition Wang et al., TPAMI 2021  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level (must be increasing)</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all pyramid levels)</p> <code>256</code> <code>num_levels</code> <code>int</code> <p>Number of output pyramid levels to generate</p> <code>5</code> <code>pooling_type</code> <code>Literal['max', 'avg']</code> <p>Pooling operation for generating lower resolution levels ('max' or 'avg')</p> <code>'avg'</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to convolution layers</p> <code>None</code> <code>stride</code> <code>int</code> <p>Stride for the final 3x3 convolutions in each pyramid level</p> <code>1</code>"},{"location":"modules/#feneck.HRFPN.out_strides","title":"<code>out_strides</code>  <code>property</code>","text":"<p>Output strides for each pyramid level.</p>"},{"location":"modules/#feneck.LRFPN","title":"<code>LRFPN</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Location-Refined Feature Pyramid Network (LR-FPN).</p> <p>The first input feature is always used as F1 (shallow). All subsequent inputs are treated as higher-level features F2, F3, ..., each refined by SPIEM and CIM. The last CIM output serves as the base of the pyramid. Additional pyramid levels are built top-down (if multiple higher features are available) and with stride-2 convolutions until <code>num_levels</code> outputs are produced.</p> <p>Reference</p> <p>LR-FPN: Enhancing Remote Sensing Object Detection with Location Refined Feature Pyramid Network Li et al., 2024  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level (must be increasing)</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels)</p> <code>256</code> <code>num_levels</code> <code>int | None</code> <p>Total number of output pyramid levels (&gt;= 1). Defaults to len(in_channels) - 1 + 2 (paper's setting).</p> <code>None</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to ConvNormLayer</p> <code>None</code>"},{"location":"modules/#feneck.NASFPN","title":"<code>NASFPN</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Neural Architecture Search Feature Pyramid Network.</p> <p>NAS-FPN uses learned feature fusion patterns discovered through neural architecture search. It employs dynamic feature recycling and channel attention for efficient multi-scale feature fusion. Generates 5 pyramid levels (P3-P7) regardless of input configuration.</p> <p>Reference</p> <p>NAS-FPN: Learning Scalable Feature Pyramid Architecture for Object Detection Ghiasi et al., CVPR 2019  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels)</p> <code>256</code> <code>num_repeats</code> <code>int</code> <p>Number of NAS-FPN cells to stack</p> <code>5</code> <code>norm_type</code> <code>str | None</code> <p>Normalization type applied to convolution layers</p> <code>'batch'</code>"},{"location":"modules/#feneck.PAFPN","title":"<code>PAFPN</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Path Aggregation Feature Pyramid Network neck module.</p> <p>PAFPN enhances FPN by adding a bottom-up path augmentation that shortens the information path between lower and top feature levels. This preserves fine-grained localization information through direct connections after the initial FPN processing.</p> <p>Reference</p> <p>Path Aggregation Network for Instance Segmentation Liu et al., CVPR 2018  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>list[int]</code> <p>Number of input channels for each feature level</p> required <code>in_strides</code> <code>list[int]</code> <p>Stride values for each input feature level (must be increasing)</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all levels)</p> <code>256</code> <code>num_levels</code> <code>int | None</code> <p>Total number of output pyramid levels. If None, uses len(in_channels). Must be &gt;= len(in_channels)</p> <code>None</code> <code>has_extra_convs</code> <code>bool</code> <p>Whether to add extra conv layers for additional pyramid levels. False uses max pooling for one extra level (Faster R-CNN style). True uses strided convolutions for extra levels (RetinaNet/FCOS style)</p> <code>False</code> <code>use_c5</code> <code>bool</code> <p>Whether to use backbone's highest feature (c5) as input for first extra level. False uses FPN's highest feature (p5) instead. Only affects extra convolutions</p> <code>True</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to all convolution layers</p> <code>None</code> <code>relu_before_extra_convs</code> <code>bool</code> <p>Whether to apply ReLU activation before extra convolutions</p> <code>True</code>"},{"location":"modules/#feneck.SimpleFPN","title":"<code>SimpleFPN</code>","text":"<p>               Bases: <code>BaseNeck</code></p> <p>Simple Feature Pyramid Network for transformer-based backbones.</p> <p>SimpleFPN is designed for non-hierarchical backbones (like Vision Transformers) that output single-scale features. It creates a feature pyramid by applying convolutions at different strides to generate multi-scale representations.</p> <p>Reference</p> <p>Exploring Plain Vision Transformer Backbones for Object Detection Li et al., ECCV 2022  Paper</p> <p>Parameters:</p> Name Type Description Default <code>in_channels</code> <code>int | list[int]</code> <p>Number of input channels (int or single-element list)</p> required <code>in_strides</code> <code>int | list[int]</code> <p>Input stride (int or single-element list)</p> required <code>out_channels</code> <code>int</code> <p>Number of output channels (same for all pyramid levels)</p> <code>256</code> <code>num_levels</code> <code>int</code> <p>Number of pyramid levels to generate</p> <code>5</code> <code>start_level</code> <code>int</code> <p>Index of pyramid level that matches input resolution. Levels below this index will have higher resolution (upsampled), levels above will have lower resolution (downsampled)</p> <code>2</code> <code>norm_type</code> <code>Literal['batch', 'group'] | None</code> <p>Normalization type applied to all convolution layers</p> <code>None</code> Usage Examples <pre><code>&gt;&gt;&gt; # ViT outputs 14x14 features at stride 16\n&gt;&gt;&gt; # We want 5 levels: stride 4, 8, 16, 32, 64\n&gt;&gt;&gt; # Input stride 16 matches level 2, so start_level=2\n&gt;&gt;&gt; neck = SimpleFPN(\n...     in_channels=768, in_strides=16, out_channels=256,\n...     num_levels=5, start_level=2\n... )\n&gt;&gt;&gt; # Output: [stride4, stride8, stride16, stride32, stride64]\n&gt;&gt;&gt; #         [level0,  level1, level2,   level3,   level4]\n&gt;&gt;&gt; #         [upsample, upsample, same, downsample, downsample]\n</code></pre>"}]}